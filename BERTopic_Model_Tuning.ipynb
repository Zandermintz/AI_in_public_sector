{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installing necessary libraries for OpenAlex API and BERTopic pipeline\n",
    "\n",
    "%pip install bertopic #BERTopic:\n",
    "%pip install sentence-transformers #Sentence transformers\n",
    "%pip install spacy #Spacy\n",
    "%pip install hdbscan #HDBSCAN\n",
    "%pip install umap-learn #UMAP\n",
    "%pip install sklearn #SKlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Using BERTopic for Topic Modeling\n",
    "#check supporting documentation: https://maartengr.github.io/BERTopic/index.html\n",
    "\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import MaximalMarginalRelevance\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "\n",
    "\n",
    "# Step 1 - Extract embeddings\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Step 2 - Reduce dimensionality\n",
    "umap_model = UMAP(n_neighbors=5, n_components=3, min_dist=0.2, metric='cosine', low_memory=False)\n",
    "\n",
    "# Step 3 - Cluster reduced embeddings. lever to adjust number of clusters / topics\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=5, metric='euclidean', cluster_selection_method='leaf', prediction_data=True)\n",
    "\n",
    "# Step 4 - Tokenize topics\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "# Step 5 - Create topic representation\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "\n",
    "# Step 6 - (Optional) Fine-tune topic representations with MMR. Decreases redundancy and improves diversity of keywords.\n",
    "representation_model = MaximalMarginalRelevance(diversity=0.5)\n",
    "\n",
    "# All steps together\n",
    "topic_model = BERTopic(\n",
    "  #min_topic_size = 10,\n",
    "  embedding_model=embedding_model,          # Step 1 - Extract embeddings\n",
    "  umap_model=umap_model,                    # Step 2 - Reduce dimensionality\n",
    "  hdbscan_model=hdbscan_model,              # Step 3 - Cluster reduced embeddings\n",
    "  vectorizer_model=vectorizer_model,        # Step 4 - Tokenize topics\n",
    "  ctfidf_model=ctfidf_model,                # Step 5 - Create topic representation\n",
    "  #seed_topic_list = keywords_list,          # Step 5a - Add keyword list based on KeyBERT output & finetuning\n",
    "  representation_model=representation_model,# Step 6 - (Optional) Fine-tune topic represenation \n",
    "  calculate_probabilities=True,        \n",
    "  verbose=True\n",
    ")\n",
    "\n",
    "##Saving the model\n",
    "model_path = \"BERT_AI_Policy_topic_model\"\n",
    "topic_model.save(model_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
